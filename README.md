# Awesome-Unified-Multimodal

All the papers listed in this project come from my usual reading.
If you have found some new and interesting papers, I would appreciate it if you let me know! You can contact me via my email address: niuyuwei04@gmail.com 

I am always on the lookout for potential opportunities for discussion and collaboration, particularly internships within the industry. If you have an interest in unifying multimodal models, please do not hesitate to reach out to me.

+ [2023-08-12] SEED: Planting a SEED of Vision in Large Language Model
  [![Static Badge](https://img.shields.io/badge/2307.08041-red?logo=arxiv)](https://arxiv.org/abs/2307.08041) [![Static Badge](https://img.shields.io/badge/SEED-black?logo=github)](https://github.com/AILab-CVC/SEED)

+ [2024-03-22] LaVIT: Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization
  [![Static Badge](https://img.shields.io/badge/2309.04669-red?logo=arxiv)](https://arxiv.org/abs/2309.04669) [![Static Badge](https://img.shields.io/badge/LaVIT-black?logo=github)](https://github.com/jy0205/LaVIT)

+ [2024-04-22] SEED-X: Multimodal Models with Unified Multi-granularity Comprehension and Generation
  [![Static Badge](https://img.shields.io/badge/2404.14396-red?logo=arxiv)](https://arxiv.org/abs/2404.14396) [![Static Badge](https://img.shields.io/badge/SEED-X-black?logo=github)](https://github.com/AILab-CVC/SEED-X)

+ [2024-05-08] Emu: Generative Pretraining in Multimodality
  [![Static Badge](https://img.shields.io/badge/2307.05222-red?logo=arxiv)](https://arxiv.org/abs/2307.05222) [![Static Badge](https://img.shields.io/badge/Emu-black?logo=github)](https://github.com/baaivision/Emu)

+ [2024-05-08] Emu2: Generative Multimodal Models are In-Context Learners
  [![Static Badge](https://img.shields.io/badge/2312.13286-red?logo=arxiv)](https://arxiv.org/abs/2312.13286) [![Static Badge](https://img.shields.io/badge/Emu2-black?logo=github)](https://github.com/baaivision/Emu2)

+ [2024-05-16] Chameleon: Mixed-Modal Early-Fusion Foundation Models
  [![Static Badge](https://img.shields.io/badge/2405.09818-red?logo=arxiv)](https://arxiv.org/abs/2405.09818) [![Static Badge](https://img.shields.io/badge/Chameleon-black?logo=github)](https://github.com/facebookresearch/chameleon)

+ [2024-08-20] Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model
  [![Static Badge](https://img.shields.io/badge/2408.11039-red?logo=arxiv)](https://arxiv.org/abs/2408.11039) [![Static Badge](https://img.shields.io/badge/Transfusion-lucidrains-black?logo=github)](https://github.com/lucidrains/transfusion-pytorch)

+ [2024-09-27] Emu3: Next-Token Prediction is All You Need
  [![Static Badge](https://img.shields.io/badge/2409.18869-red?logo=arxiv)](https://arxiv.org/abs/2409.18869) [![Static Badge](https://img.shields.io/badge/Emu3-black?logo=github)](https://github.com/baaivision/Emu3)

+ [2024-10-15] MMAR: Towards Lossless Multi-Modal Auto-Regressive Probabilistic Modeling
  [![Static Badge](https://img.shields.io/badge/2410.10798-red?logo=arxiv)](https://arxiv.org/abs/2410.10798) [![Static Badge](https://img.shields.io/badge/MMAR-black?logo=github)](https://github.com/ydcUstc/MMAR)

+ [2024-10-17] Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation
  [![Static Badge](https://img.shields.io/badge/2410.13848-red?logo=arxiv)](https://arxiv.org/abs/2410.13848) [![Static Badge](https://img.shields.io/badge/Janus-black?logo=github)](https://github.com/deepseek-ai/Janus)

+ [2024-10-21] PUMA: Empowering Unified MLLM with Multi-granular Visual Generation
  [![Static Badge](https://img.shields.io/badge/2410.13861-red?logo=arxiv)](https://arxiv.org/abs/2410.13861) [![Static Badge](https://img.shields.io/badge/PUMA-black?logo=github)](https://github.com/rongyaofang/PUMA)

+ [2024-10-21] Show-o: One Single Transformer to Unify Multimodal Understanding and Generation
  [![Static Badge](https://img.shields.io/badge/2408.12528-red?logo=arxiv)](https://arxiv.org/abs/2408.12528) [![Static Badge](https://img.shields.io/badge/Show-o-black?logo=github)](https://github.com/showlab/Show-o)

+ [2024-10-23] VILA-U: A Unified Foundation Model Integrating Visual Understanding and Generation
  [![Static Badge](https://img.shields.io/badge/2409.04429-red?logo=arxiv)](https://arxiv.org/abs/2409.04429) [![Static Badge](https://img.shields.io/badge/VILA-U-black?logo=github)](https://github.com/mit-han-lab/vila-u)

+ [2024-10-31] MIO: A Foundation Model on Multimodal Tokens
  [![Static Badge](https://img.shields.io/badge/2409.17692-red?logo=arxiv)](https://arxiv.org/abs/2409.17692) [![Static Badge](https://img.shields.io/badge/MIO-black?logo=github)](https://github.com/MIO-Team/MIO)

+ [2024-11-12] JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation
  [![Static Badge](https://img.shields.io/badge/2411.07975-red?logo=arxiv)](https://arxiv.org/abs/2411.07975) [![Static Badge](https://img.shields.io/badge/JanusFlow-black?logo=github)](https://github.com/deepseek-ai/Janus)

+ [2024-11-28] Orthus: Autoregressive Interleaved Image-Text Generation with Modality-Specific Heads
    [![Static Badge](https://img.shields.io/badge/2412.00127-red?logo=arxiv)](https://arxiv.org/abs/2412.00127)


+ [2024-12-04] TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation
  [![Static Badge](https://img.shields.io/badge/2412.03069-red?logo=arxiv)](https://arxiv.org/abs/2412.03069) [![Static Badge](https://img.shields.io/badge/TokenFlow-black?logo=github)](https://github.com/ByteFlow-AI/TokenFlow)

+ [2024-12-05] Liquid: Language Models are Scalable Multi-modal Generators
  [![Static Badge](https://img.shields.io/badge/2412.04332-red?logo=arxiv)](https://arxiv.org/abs/2412.04332)

+ [2024-12-05] MUSE-VL: Modeling Unified VLM through Semantic Discrete Encoding
  [![Static Badge](https://img.shields.io/badge/2411.17762-red?logo=arxiv)](https://arxiv.org/abs/2411.17762)

+ [2024-12-08] SILMM: Self-Improving Large Multimodal Models for Compositional Text-to-Image Generation
  [![Static Badge](https://img.shields.io/badge/2412.05818-red?logo=arxiv)](https://arxiv.org/abs/2412.05818)

+ [2024-12-09] ILLUME: Illuminating Your LLMs to See, Draw, and Self-Enhance
  [![Static Badge](https://img.shields.io/badge/2412.06673-red?logo=arxiv)](https://arxiv.org/abs/2412.06673)

+ [2024-12-09] Visual Lexicon: Rich Image Features in Language Space
  [![Static Badge](https://img.shields.io/badge/2412.06774-red?logo=arxiv)](https://arxiv.org/abs/2412.06774)

+ [2024-12-11] Multimodal Latent Language Modeling with Next-Token Diffusion
  [![Static Badge](https://img.shields.io/badge/2412.08635-red?logo=arxiv)](https://arxiv.org/abs/2412.08635)

+ [2024-12-12] SynerGen-VL: Towards Synergistic Image Understanding and Generation with Vision Experts and Token Folding
  [![Static Badge](https://img.shields.io/badge/2412.09604-red?logo=arxiv)](https://arxiv.org/abs/2412.09604)

+ [2024-12-18] MetaMorph: Multimodal Understanding and Generation via Instruction Tuning
  [![Static Badge](https://img.shields.io/badge/2412.14164-red?logo=arxiv)](https://arxiv.org/abs/2412.14164)
  
+ [2024-12-26] LMFusion: Adapting Pretrained Language Models for Multimodal Generation
  [![Static Badge](https://img.shields.io/badge/2412.15188-red?logo=arxiv)](https://arxiv.org/abs/2412.15188)
  
+ [2024-12-31] Dual Diffusion for Unified Image Generation and Understanding
  [![Static Badge](https://img.shields.io/badge/2501.00289-red?logo=arxiv)](https://arxiv.org/abs/2501.00289)

+ [2025-1-21] VARGPT: Unified Understanding and Generation in a Visual Autoregressive Multimodal Large Language Model
  [![Static Badge](https://img.shields.io/badge/2501.12327-red?logo=arxiv)](https://arxiv.org/abs/2501.12327)

+ [2025-1-21] Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling
  [![Static Badge](https://img.shields.io/badge/2501.17811-red?logo=arxiv)](https://arxiv.org/abs/2501.17811)

+ [2025-2-7] QLIP: Text-Aligned Visual Tokenization Unifies Auto-Regressive Multimodal Understanding and Generation
  [![Static Badge](https://img.shields.io/badge/2502.05178-red?logo=arxiv)](https://arxiv.org/abs/2502.05178)

+ [2025-2-17] HermesFlow: Seamlessly Closing the Gap in Multimodal Understanding and Generation
  [![Static Badge](https://img.shields.io/badge/2502.12148-red?logo=arxiv)](https://arxiv.org/abs/2502.12148)

+ [2025-2-27] UniTok: A Unified Tokenizer for Visual Generation and Understanding
  [![Static Badge](https://img.shields.io/badge/2502.20321-red?logo=arxiv)](https://arxiv.org/abs/2502.20321)

+ [2025-3-9] SemHiTok: A Unified Image Tokenizer via Semantic-Guided Hierarchical Codebook for Multimodal Understanding and Generation
  [![Static Badge](https://img.shields.io/badge/2503.06764-red?logo=arxiv)](https://arxiv.org/abs/2503.06764)

+ [2025-3-10] WISE: A World Knowledge-Informed Semantic Evaluation for Text-to-Image Generation  
 **The only study that examines whether understanding benefits generation at the level of world knowledge, highly recommended! (since it's also my work, lol)**
  [![Static Badge](https://img.shields.io/badge/2503.07265-red?logo=arxiv)](https://arxiv.org/abs/2503.07265)

+ [2025-3-18] Unified Autoregressive Visual Generation and Understanding with Continuous Tokens  
  [![Static Badge](https://img.shields.io/badge/2503.13436-red?logo=arxiv)](https://arxiv.org/abs/2503.13436)

+ [2024-3-19] DualToken: Towards Unifying Visual Understanding and Generation with Dual Visual Vocabularies  
  [![Static Badge](https://img.shields.io/badge/2503.14324-red?logo=arxiv)](https://arxiv.org/abs/2503.14324)
  
# Useful Links

+ [Awesome-Unified-Multimodal-Models](https://github.com/showlab/Awesome-Unified-Multimodal-Models)
