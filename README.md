# üåü Awesome Unified Multimodal Models

Welcome to **Awesome-Unified-Multimodal**! This curated collection features cutting-edge papers on unified multimodal models, sourced from my regular readings. I‚Äôm passionate about advancing research in this field and am always eager to discover new, exciting work.  

‚ú® **Contributions Welcome**: If you‚Äôve come across intriguing papers, please share them with me at [niuyuwei04@gmail.com](mailto:niuyuwei04@gmail.com).  
ü§ù **Collaboration Opportunities**: I‚Äôm actively seeking discussions and industry internships focused on unifying multimodal models. Feel free to reach out if you share this interest!

---

## üìë Table of Contents
- [2023](#2023)
- [2024](#2024)
- [2025](#2025)
- [Useful Links](#useful-links)

---

## üìö 2023

- **[2023-08-12]** **SEED: Planting a SEED of Vision in Large Language Model**  
  [![ArXiv](https://img.shields.io/badge/2307.08041-red?logo=arxiv)](https://arxiv.org/abs/2307.08041) [![GitHub](https://img.shields.io/badge/SEED-black?logo=github)](https://github.com/AILab-CVC/SEED)

---

## üìö 2024

- **[2024-03-22]** **LaVIT: Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization**  
  [![ArXiv](https://img.shields.io/badge/2309.04669-red?logo=arxiv)](https://arxiv.org/abs/2309.04669) [![GitHub](https://img.shields.io/badge/LaVIT-black?logo=github)](https://github.com/jy0205/LaVIT)

- **[2024-04-22]** **SEED-X: Multimodal Models with Unified Multi-granularity Comprehension and Generation**  
  [![ArXiv](https://img.shields.io/badge/2404.14396-red?logo=arxiv)](https://arxiv.org/abs/2404.14396) [![GitHub](https://img.shields.io/badge/SEED--X-black?logo=github)](https://github.com/AILab-CVC/SEED-X)

- **[2024-05-08]** **Emu: Generative Pretraining in Multimodality**  
  [![ArXiv](https://img.shields.io/badge/2307.05222-red?logo=arxiv)](https://arxiv.org/abs/2307.05222) [![GitHub](https://img.shields.io/badge/Emu-black?logo=github)](https://github.com/baaivision/Emu)

- **[2024-05-08]** **Emu2: Generative Multimodal Models are In-Context Learners**  
  [![ArXiv](https://img.shields.io/badge/2312.13286-red?logo=arxiv)](https://arxiv.org/abs/2312.13286) [![GitHub](https://img.shields.io/badge/Emu2-black?logo=github)](https://github.com/baaivision/Emu2)

- **[2024-05-16]** **Chameleon: Mixed-Modal Early-Fusion Foundation Models**  
  [![ArXiv](https://img.shields.io/badge/2405.09818-red?logo=arxiv)](https://arxiv.org/abs/2405.09818) [![GitHub](https://img.shields.io/badge/Chameleon-black?logo=github)](https://github.com/facebookresearch/chameleon)

- **[2024-08-20]** **Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model**  
  [![ArXiv](https://img.shields.io/badge/2408.11039-red?logo=arxiv)](https://arxiv.org/abs/2408.11039) [![GitHub](https://img.shields.io/badge/Transfusion-black?logo=github)](https://github.com/lucidrains/transfusion-pytorch)

- **[2024-09-27]** **Emu3: Next-Token Prediction is All You Need**  
  [![ArXiv](https://img.shields.io/badge/2409.18869-red?logo=arxiv)](https://arxiv.org/abs/2409.18869) [![GitHub](https://img.shields.io/badge/Emu3-black?logo=github)](https://github.com/baaivision/Emu3)

- **[2024-10-15]** **MMAR: Towards Lossless Multi-Modal Auto-Regressive Probabilistic Modeling**  
  [![ArXiv](https://img.shields.io/badge/2410.10798-red?logo=arxiv)](https://arxiv.org/abs/2410.10798) [![GitHub](https://img.shields.io/badge/MMAR-black?logo=github)](https://github.com/ydcUstc/MMAR)

- **[2024-10-17]** **Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation**  
  [![ArXiv](https://img.shields.io/badge/2410.13848-red?logo=arxiv)](https://arxiv.org/abs/2410.13848) [![GitHub](https://img.shields.io/badge/Janus-black?logo=github)](https://github.com/deepseek-ai/Janus)

- **[2024-10-21]** **PUMA: Empowering Unified MLLM with Multi-granular Visual Generation**  
  [![ArXiv](https://img.shields.io/badge/2410.13861-red?logo=arxiv)](https://arxiv.org/abs/2410.13861) [![GitHub](https://img.shields.io/badge/PUMA-black?logo=github)](https://github.com/rongyaofang/PUMA)

- **[2024-10-21]** **Show-o: One Single Transformer to Unify Multimodal Understanding and Generation**  
  [![ArXiv](https://img.shields.io/badge/2408.12528-red?logo=arxiv)](https://arxiv.org/abs/2408.12528) [![GitHub](https://img.shields.io/badge/Show--o-black?logo=github)](https://github.com/showlab/Show-o)

- **[2024-10-23]** **VILA-U: A Unified Foundation Model Integrating Visual Understanding and Generation**  
  [![ArXiv](https://img.shields.io/badge/2409.04429-red?logo=arxiv)](https://arxiv.org/abs/2409.04429) [![GitHub](https://img.shields.io/badge/VILA--U-black?logo=github)](https://github.com/mit-han-lab/vila-u)

- **[2024-10-31]** **MIO: A Foundation Model on Multimodal Tokens**  
  [![ArXiv](https://img.shields.io/badge/2409.17692-red?logo=arxiv)](https://arxiv.org/abs/2409.17692) [![GitHub](https://img.shields.io/badge/MIO-black?logo=github)](https://github.com/MIO-Team/MIO)

- **[2024-11-12]** **JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation**  
  [![ArXiv](https://img.shields.io/badge/2411.07975-red?logo=arxiv)](https://arxiv.org/abs/2411.07975) [![GitHub](https://img.shields.io/badge/JanusFlow-black?logo=github)](https://github.com/deepseek-ai/Janus)

- **[2024-11-28]** **Orthus: Autoregressive Interleaved Image-Text Generation with Modality-Specific Heads**  
  [![ArXiv](https://img.shields.io/badge/2412.00127-red?logo=arxiv)](https://arxiv.org/abs/2412.00127)

- **[2024-12-04]** **TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation**  
  [![ArXiv](https://img.shields.io/badge/2412.03069-red?logo=arxiv)](https://arxiv.org/abs/2412.03069) [![GitHub](https://img.shields.io/badge/TokenFlow-black?logo=github)](https://github.com/ByteFlow-AI/TokenFlow)

- **[2024-12-05]** **Liquid: Language Models are Scalable Multi-modal Generators**  
  [![ArXiv](https://img.shields.io/badge/2412.04332-red?logo=arxiv)](https://arxiv.org/abs/2412.04332)

- **[2024-12-05]** **MUSE-VL: Modeling Unified VLM through Semantic Discrete Encoding**  
  [![ArXiv](https://img.shields.io/badge/2411.17762-red?logo=arxiv)](https://arxiv.org/abs/2411.17762)

- **[2024-12-08]** **SILMM: Self-Improving Large Multimodal Models for Compositional Text-to-Image Generation**  
  [![ArXiv](https://img.shields.io/badge/2412.05818-red?logo=arxiv)](https://arxiv.org/abs/2412.05818)

- **[2024-12-09]** **ILLUME: Illuminating Your LLMs to See, Draw, and Self-Enhance**  
  [![ArXiv](https://img.shields.io/badge/2412.06673-red?logo=arxiv)](https://arxiv.org/abs/2412.06673)

- **[2024-12-09]** **Visual Lexicon: Rich Image Features in Language Space**  
  [![ArXiv](https://img.shields.io/badge/2412.06774-red?logo=arxiv)](https://arxiv.org/abs/2412.06774)

- **[2024-12-11]** **Multimodal Latent Language Modeling with Next-Token Diffusion**  
  [![ArXiv](https://img.shields.io/badge/2412.08635-red?logo=arxiv)](https://arxiv.org/abs/2412.08635)

- **[2024-12-12]** **SynerGen-VL: Towards Synergistic Image Understanding and Generation with Vision Experts and Token Folding**  
  [![ArXiv](https://img.shields.io/badge/2412.09604-red?logo=arxiv)](https://arxiv.org/abs/2412.09604)

- **[2024-12-18]** **MetaMorph: Multimodal Understanding and Generation via Instruction Tuning**  
  [![ArXiv](https://img.shields.io/badge/2412.14164-red?logo=arxiv)](https://arxiv.org/abs/2412.14164)

- **[2024-12-26]** **LMFusion: Adapting Pretrained Language Models for Multimodal Generation**  
  [![ArXiv](https://img.shields.io/badge/2412.15188-red?logo=arxiv)](https://arxiv.org/abs/2412.15188)

- **[2024-12-31]** **Dual Diffusion for Unified Image Generation and Understanding**  
  [![ArXiv](https://img.shields.io/badge/2501.00289-red?logo=arxiv)](https://arxiv.org/abs/2501.00289)

---

## üìö 2025

- **[2025-01-21]** **VARGPT: Unified Understanding and Generation in a Visual Autoregressive Multimodal Large Language Model**  
  [![ArXiv](https://img.shields.io/badge/2501.12327-red?logo=arxiv)](https://arxiv.org/abs/2501.12327)

- **[2025-01-21]** **Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling**  
  [![ArXiv](https://img.shields.io/badge/2501.17811-red?logo=arxiv)](https://arxiv.org/abs/2501.17811)

- **[2025-02-07]** **QLIP: Text-Aligned Visual Tokenization Unifies Auto-Regressive Multimodal Understanding and Generation**  
  [![ArXiv](https://img.shields.io/badge/2502.05178-red?logo=arxiv)](https://arxiv.org/abs/2502.05178)

- **[2025-02-17]** **HermesFlow: Seamlessly Closing the Gap in Multimodal Understanding and Generation**  
  [![ArXiv](https://img.shields.io/badge/2502.12148-red?logo=arxiv)](https://arxiv.org/abs/2502.12148)

- **[2025-02-27]** **UniTok: A Unified Tokenizer for Visual Generation and Understanding**  
  [![ArXiv](https://img.shields.io/badge/2502.20321-red?logo=arxiv)](https://arxiv.org/abs/2502.20321)

- **[2025-03-09]** **SemHiTok: A Unified Image Tokenizer via Semantic-Guided Hierarchical Codebook for Multimodal Understanding and Generation**  
  [![ArXiv](https://img.shields.io/badge/2503.06764-red?logo=arxiv)](https://arxiv.org/abs/2503.06764)

- **[2025-03-10]** **WISE: A World Knowledge-Informed Semantic Evaluation for Text-to-Image Generation**  
  *‚≠ê Highly recommended! The only study exploring how understanding enhances generation through world knowledge (and yes, it‚Äôs my work, lol).*  
  [![ArXiv](https://img.shields.io/badge/2503.07265-red?logo=arxiv)](https://arxiv.org/abs/2503.07265)

- **[2025-03-18]** **Unified Autoregressive Visual Generation and Understanding with Continuous Tokens**  
  [![ArXiv](https://img.shields.io/badge/2503.13436-red?logo=arxiv)](https://arxiv.org/abs/2503.13436)

- **[2025-03-19]** **DualToken: Towards Unifying Visual Understanding and Generation with Dual Visual Vocabularies**  
  [![ArXiv](https://img.shields.io/badge/2503.14324-red?logo=arxiv)](https://arxiv.org/abs/2503.14324)

---

## üîó Useful Links

- üåê **[Awesome-Unified-Multimodal-Models](https://github.com/showlab/Awesome-Unified-Multimodal-Models)** - A companion repository with more resources.  
- üìä **[SOTA-Paper-Rating](https://waynejin0918.github.io/SOTA-paper-rating.io/#)** - Check out ratings for state-of-the-art papers.

---

*Last updated: March 30, 2025*
